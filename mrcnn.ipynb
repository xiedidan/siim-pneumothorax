{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import torch.distributed as dist\n",
    "import errno\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import random\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/siim-pneumothorax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "def all_gather(data):\n",
    "    \"\"\"\n",
    "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
    "    Args:\n",
    "        data: any picklable object\n",
    "    Returns:\n",
    "        list[data]: list of data gathered from each rank\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "    # serialized to a Tensor\n",
    "    buffer = pickle.dumps(data)\n",
    "    storage = torch.ByteStorage.from_buffer(buffer)\n",
    "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
    "\n",
    "    # obtain Tensor size of each rank\n",
    "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
    "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
    "    dist.all_gather(size_list, local_size)\n",
    "    size_list = [int(size.item()) for size in size_list]\n",
    "    max_size = max(size_list)\n",
    "\n",
    "    # receiving Tensor from all ranks\n",
    "    # we pad the tensor because torch all_gather does not support\n",
    "    # gathering tensors of different shapes\n",
    "    tensor_list = []\n",
    "    for _ in size_list:\n",
    "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
    "    if local_size != max_size:\n",
    "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    dist.all_gather(tensor_list, tensor)\n",
    "\n",
    "    data_list = []\n",
    "    for size, tensor in zip(size_list, tensor_list):\n",
    "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
    "        data_list.append(pickle.loads(buffer))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.no_grad():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "    return reduced_dict\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = self.delimiter.join([\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}',\n",
    "            'max mem: {memory:.0f}'\n",
    "        ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                print(log_msg.format(\n",
    "                    i, len(iterable), eta=eta_string,\n",
    "                    meters=str(self),\n",
    "                    time=str(iter_time), data=str(data_time),\n",
    "                    memory=torch.cuda.max_memory_allocated() / MB))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}'.format(\n",
    "        args.rank, args.dist_url), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle2mask(rle, width, height):\n",
    "    mask= np.zeros(width* height)\n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]\n",
    "    lengths = array[1::2]\n",
    "\n",
    "    current_position = 0\n",
    "    for index, start in enumerate(starts):\n",
    "        current_position += start\n",
    "        mask[current_position:current_position+lengths[index]] = 1\n",
    "        current_position += lengths[index]\n",
    "\n",
    "    return mask.reshape(width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIIMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, img_dir):\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.height = 1024\n",
    "        self.width = 1024\n",
    "        self.image_dir = img_dir\n",
    "        self.image_info = collections.defaultdict(dict)\n",
    "\n",
    "        counter = 0\n",
    "        for index, row in tqdm(self.df.iterrows(), total=len(self.df)):\n",
    "            image_id = row['ImageId']\n",
    "            image_path = os.path.join(self.image_dir, image_id)\n",
    "            if os.path.exists(image_path + '.png') and row[\" EncodedPixels\"].strip() != \"-1\":\n",
    "                self.image_info[counter][\"image_id\"] = image_id\n",
    "                self.image_info[counter][\"image_path\"] = image_path\n",
    "                self.image_info[counter][\"annotations\"] = row[\" EncodedPixels\"].strip()\n",
    "                counter += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_info[idx][\"image_path\"]\n",
    "        img = Image.open(img_path + '.png').convert(\"RGB\")\n",
    "        width, height = img.size\n",
    "        img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n",
    "        info = self.image_info[idx]\n",
    "\n",
    "        mask = rle2mask(info['annotations'], width, height)\n",
    "        mask = Image.fromarray(mask.T)\n",
    "        mask = mask.resize((self.width, self.height), resample=Image.BILINEAR)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "\n",
    "        pos = np.where(np.array(mask)[0, :, :])\n",
    "        xmin = np.min(pos[1])\n",
    "        xmax = np.max(pos[1])\n",
    "        ymin = np.min(pos[0])\n",
    "        ymax = np.max(pos[0])\n",
    "\n",
    "        boxes = torch.as_tensor([[xmin, ymin, xmax, ymax]], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        img = transforms.ToTensor()(img)\n",
    "        \n",
    "        if random.random() < 0.8:\n",
    "            height, width = img.shape[-2:]\n",
    "            img = img.flip(-1)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "            target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11582/11582 [00:00<00:00, 13735.63it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = SIIMDataset(\n",
    "    os.path.join(data_path, \"train-rle.csv\"),\n",
    "    os.path.join(data_path, \"train_png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3286"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask rcnn model\n",
    "\n",
    "num_classes = 2\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "model_ft = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "in_features = model_ft.roi_heads.box_predictor.cls_score.in_features\n",
    "model_ft.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "in_features_mask = model_ft.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model_ft.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "model_ft.to(device)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=4, shuffle=True, num_workers=8,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model_ft.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=5,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/822]  eta: 0:27:15  lr: 0.000002  loss: 1.8789 (1.8789)  loss_classifier: 0.6265 (0.6265)  loss_box_reg: 0.0574 (0.0574)  loss_mask: 1.0987 (1.0987)  loss_objectness: 0.0882 (0.0882)  loss_rpn_box_reg: 0.0081 (0.0081)  time: 1.9897  data: 1.0338  max mem: 6085\n",
      "Epoch: [0]  [100/822]  eta: 0:06:32  lr: 0.000124  loss: 0.6926 (1.1535)  loss_classifier: 0.0915 (0.2071)  loss_box_reg: 0.0476 (0.0361)  loss_mask: 0.5055 (0.7949)  loss_objectness: 0.0351 (0.1031)  loss_rpn_box_reg: 0.0082 (0.0122)  time: 0.5321  data: 0.0085  max mem: 8011\n",
      "Epoch: [0]  [200/822]  eta: 0:05:36  lr: 0.000246  loss: 0.6382 (0.9272)  loss_classifier: 0.0928 (0.1524)  loss_box_reg: 0.0497 (0.0448)  loss_mask: 0.4191 (0.6428)  loss_objectness: 0.0288 (0.0760)  loss_rpn_box_reg: 0.0075 (0.0112)  time: 0.5362  data: 0.0092  max mem: 8037\n",
      "Epoch: [0]  [300/822]  eta: 0:04:43  lr: 0.000367  loss: 0.5954 (0.8333)  loss_classifier: 0.0895 (0.1324)  loss_box_reg: 0.0557 (0.0488)  loss_mask: 0.3774 (0.5767)  loss_objectness: 0.0255 (0.0637)  loss_rpn_box_reg: 0.0068 (0.0118)  time: 0.5649  data: 0.0084  max mem: 8056\n",
      "Epoch: [0]  [400/822]  eta: 0:03:48  lr: 0.000489  loss: 0.5872 (0.7763)  loss_classifier: 0.0860 (0.1217)  loss_box_reg: 0.0512 (0.0509)  loss_mask: 0.4243 (0.5368)  loss_objectness: 0.0253 (0.0555)  loss_rpn_box_reg: 0.0058 (0.0114)  time: 0.5353  data: 0.0083  max mem: 8056\n",
      "Epoch: [0]  [500/822]  eta: 0:02:54  lr: 0.000611  loss: 0.5665 (0.7390)  loss_classifier: 0.0945 (0.1154)  loss_box_reg: 0.0628 (0.0524)  loss_mask: 0.3821 (0.5111)  loss_objectness: 0.0153 (0.0492)  loss_rpn_box_reg: 0.0058 (0.0109)  time: 0.5368  data: 0.0085  max mem: 8056\n",
      "Epoch: [0]  [600/822]  eta: 0:02:00  lr: 0.000732  loss: 0.5595 (0.7150)  loss_classifier: 0.0895 (0.1110)  loss_box_reg: 0.0550 (0.0527)  loss_mask: 0.3797 (0.4948)  loss_objectness: 0.0241 (0.0460)  loss_rpn_box_reg: 0.0058 (0.0106)  time: 0.5310  data: 0.0085  max mem: 8056\n",
      "Epoch: [0]  [700/822]  eta: 0:01:05  lr: 0.000854  loss: 0.5365 (0.6935)  loss_classifier: 0.0921 (0.1080)  loss_box_reg: 0.0534 (0.0526)  loss_mask: 0.3659 (0.4798)  loss_objectness: 0.0127 (0.0429)  loss_rpn_box_reg: 0.0063 (0.0102)  time: 0.5381  data: 0.0085  max mem: 8056\n",
      "Epoch: [0]  [800/822]  eta: 0:00:11  lr: 0.000976  loss: 0.4824 (0.6801)  loss_classifier: 0.0762 (0.1052)  loss_box_reg: 0.0396 (0.0519)  loss_mask: 0.3459 (0.4716)  loss_objectness: 0.0143 (0.0411)  loss_rpn_box_reg: 0.0056 (0.0102)  time: 0.5379  data: 0.0084  max mem: 8056\n",
      "Epoch: [0]  [821/822]  eta: 0:00:00  lr: 0.001000  loss: 0.5195 (0.6767)  loss_classifier: 0.0808 (0.1047)  loss_box_reg: 0.0425 (0.0517)  loss_mask: 0.3558 (0.4695)  loss_objectness: 0.0215 (0.0407)  loss_rpn_box_reg: 0.0059 (0.0102)  time: 0.5172  data: 0.0084  max mem: 8056\n",
      "Epoch: [0] Total time: 0:07:23 (0.5392 s / it)\n",
      "Epoch: [1]  [  0/822]  eta: 0:20:35  lr: 0.001000  loss: 0.5169 (0.5169)  loss_classifier: 0.0668 (0.0668)  loss_box_reg: 0.0392 (0.0392)  loss_mask: 0.3961 (0.3961)  loss_objectness: 0.0094 (0.0094)  loss_rpn_box_reg: 0.0055 (0.0055)  time: 1.5029  data: 0.9554  max mem: 8056\n",
      "Epoch: [1]  [100/822]  eta: 0:06:31  lr: 0.001000  loss: 0.5205 (0.5258)  loss_classifier: 0.0835 (0.0858)  loss_box_reg: 0.0458 (0.0464)  loss_mask: 0.3636 (0.3676)  loss_objectness: 0.0130 (0.0184)  loss_rpn_box_reg: 0.0052 (0.0076)  time: 0.5314  data: 0.0085  max mem: 8056\n",
      "Epoch: [1]  [200/822]  eta: 0:05:35  lr: 0.001000  loss: 0.5051 (0.5242)  loss_classifier: 0.0809 (0.0872)  loss_box_reg: 0.0456 (0.0468)  loss_mask: 0.3392 (0.3615)  loss_objectness: 0.0182 (0.0207)  loss_rpn_box_reg: 0.0066 (0.0080)  time: 0.5302  data: 0.0084  max mem: 8058\n",
      "Epoch: [1]  [300/822]  eta: 0:04:40  lr: 0.001000  loss: 0.5379 (0.5289)  loss_classifier: 0.0866 (0.0877)  loss_box_reg: 0.0433 (0.0467)  loss_mask: 0.3534 (0.3658)  loss_objectness: 0.0147 (0.0205)  loss_rpn_box_reg: 0.0067 (0.0081)  time: 0.5363  data: 0.0084  max mem: 8058\n",
      "Epoch: [1]  [400/822]  eta: 0:03:46  lr: 0.001000  loss: 0.4820 (0.5232)  loss_classifier: 0.0789 (0.0867)  loss_box_reg: 0.0445 (0.0462)  loss_mask: 0.3069 (0.3610)  loss_objectness: 0.0137 (0.0211)  loss_rpn_box_reg: 0.0058 (0.0083)  time: 0.5349  data: 0.0086  max mem: 8058\n",
      "Epoch: [1]  [500/822]  eta: 0:02:52  lr: 0.001000  loss: 0.5331 (0.5243)  loss_classifier: 0.0964 (0.0873)  loss_box_reg: 0.0476 (0.0463)  loss_mask: 0.3560 (0.3621)  loss_objectness: 0.0158 (0.0206)  loss_rpn_box_reg: 0.0058 (0.0080)  time: 0.5445  data: 0.0083  max mem: 8082\n",
      "Epoch: [1]  [600/822]  eta: 0:01:59  lr: 0.001000  loss: 0.4517 (0.5189)  loss_classifier: 0.0849 (0.0872)  loss_box_reg: 0.0427 (0.0464)  loss_mask: 0.3015 (0.3572)  loss_objectness: 0.0141 (0.0202)  loss_rpn_box_reg: 0.0056 (0.0079)  time: 0.5380  data: 0.0085  max mem: 8082\n",
      "Epoch: [1]  [700/822]  eta: 0:01:05  lr: 0.001000  loss: 0.4651 (0.5155)  loss_classifier: 0.0804 (0.0864)  loss_box_reg: 0.0468 (0.0460)  loss_mask: 0.3143 (0.3548)  loss_objectness: 0.0176 (0.0203)  loss_rpn_box_reg: 0.0068 (0.0081)  time: 0.5291  data: 0.0088  max mem: 8082\n",
      "Epoch: [1]  [800/822]  eta: 0:00:11  lr: 0.001000  loss: 0.5129 (0.5153)  loss_classifier: 0.0794 (0.0857)  loss_box_reg: 0.0432 (0.0457)  loss_mask: 0.3613 (0.3545)  loss_objectness: 0.0172 (0.0208)  loss_rpn_box_reg: 0.0061 (0.0086)  time: 0.5418  data: 0.0094  max mem: 8082\n",
      "Epoch: [1]  [821/822]  eta: 0:00:00  lr: 0.001000  loss: 0.4688 (0.5174)  loss_classifier: 0.0765 (0.0857)  loss_box_reg: 0.0420 (0.0457)  loss_mask: 0.3228 (0.3566)  loss_objectness: 0.0169 (0.0208)  loss_rpn_box_reg: 0.0069 (0.0086)  time: 0.5226  data: 0.0088  max mem: 8082\n",
      "Epoch: [1] Total time: 0:07:21 (0.5367 s / it)\n",
      "Epoch: [2]  [  0/822]  eta: 0:22:16  lr: 0.001000  loss: 0.4775 (0.4775)  loss_classifier: 0.0822 (0.0822)  loss_box_reg: 0.0529 (0.0529)  loss_mask: 0.3266 (0.3266)  loss_objectness: 0.0112 (0.0112)  loss_rpn_box_reg: 0.0045 (0.0045)  time: 1.6260  data: 0.9857  max mem: 8082\n",
      "Epoch: [2]  [100/822]  eta: 0:06:42  lr: 0.001000  loss: 0.4599 (0.5105)  loss_classifier: 0.0805 (0.0841)  loss_box_reg: 0.0432 (0.0436)  loss_mask: 0.3098 (0.3569)  loss_objectness: 0.0114 (0.0185)  loss_rpn_box_reg: 0.0047 (0.0075)  time: 0.5449  data: 0.0096  max mem: 8082\n",
      "Epoch: [2]  [200/822]  eta: 0:05:51  lr: 0.001000  loss: 0.4721 (0.5057)  loss_classifier: 0.0784 (0.0846)  loss_box_reg: 0.0439 (0.0441)  loss_mask: 0.3416 (0.3510)  loss_objectness: 0.0134 (0.0184)  loss_rpn_box_reg: 0.0073 (0.0076)  time: 0.5712  data: 0.0097  max mem: 8082\n",
      "Epoch: [2]  [300/822]  eta: 0:04:59  lr: 0.001000  loss: 0.4346 (0.4927)  loss_classifier: 0.0696 (0.0837)  loss_box_reg: 0.0452 (0.0444)  loss_mask: 0.3070 (0.3400)  loss_objectness: 0.0082 (0.0173)  loss_rpn_box_reg: 0.0061 (0.0073)  time: 0.6194  data: 0.0093  max mem: 8082\n",
      "Epoch: [2]  [400/822]  eta: 0:04:03  lr: 0.001000  loss: 0.4333 (0.4857)  loss_classifier: 0.0788 (0.0830)  loss_box_reg: 0.0405 (0.0443)  loss_mask: 0.2870 (0.3338)  loss_objectness: 0.0098 (0.0173)  loss_rpn_box_reg: 0.0053 (0.0073)  time: 0.5680  data: 0.0094  max mem: 8082\n",
      "Epoch: [2]  [500/822]  eta: 0:03:07  lr: 0.001000  loss: 0.4887 (0.4830)  loss_classifier: 0.0778 (0.0829)  loss_box_reg: 0.0442 (0.0443)  loss_mask: 0.3313 (0.3308)  loss_objectness: 0.0178 (0.0174)  loss_rpn_box_reg: 0.0055 (0.0076)  time: 0.6446  data: 0.0147  max mem: 8082\n",
      "Epoch: [2]  [600/822]  eta: 0:02:10  lr: 0.001000  loss: 0.4812 (0.4845)  loss_classifier: 0.0824 (0.0828)  loss_box_reg: 0.0432 (0.0442)  loss_mask: 0.3060 (0.3325)  loss_objectness: 0.0132 (0.0175)  loss_rpn_box_reg: 0.0059 (0.0076)  time: 0.5879  data: 0.0093  max mem: 8082\n",
      "Epoch: [2]  [700/822]  eta: 0:01:12  lr: 0.001000  loss: 0.4446 (0.4827)  loss_classifier: 0.0722 (0.0822)  loss_box_reg: 0.0365 (0.0438)  loss_mask: 0.3095 (0.3308)  loss_objectness: 0.0116 (0.0179)  loss_rpn_box_reg: 0.0045 (0.0079)  time: 0.5970  data: 0.0090  max mem: 8082\n",
      "Epoch: [2]  [800/822]  eta: 0:00:13  lr: 0.001000  loss: 0.4156 (0.4795)  loss_classifier: 0.0750 (0.0819)  loss_box_reg: 0.0401 (0.0436)  loss_mask: 0.2739 (0.3279)  loss_objectness: 0.0125 (0.0177)  loss_rpn_box_reg: 0.0061 (0.0083)  time: 0.6537  data: 0.0094  max mem: 8082\n",
      "Epoch: [2]  [821/822]  eta: 0:00:00  lr: 0.001000  loss: 0.4359 (0.4791)  loss_classifier: 0.0778 (0.0819)  loss_box_reg: 0.0402 (0.0437)  loss_mask: 0.3036 (0.3275)  loss_objectness: 0.0159 (0.0178)  loss_rpn_box_reg: 0.0059 (0.0083)  time: 0.5682  data: 0.0092  max mem: 8082\n",
      "Epoch: [2] Total time: 0:08:08 (0.5943 s / it)\n",
      "Epoch: [3]  [  0/822]  eta: 0:26:09  lr: 0.001000  loss: 0.4984 (0.4984)  loss_classifier: 0.0863 (0.0863)  loss_box_reg: 0.0548 (0.0548)  loss_mask: 0.3150 (0.3150)  loss_objectness: 0.0314 (0.0314)  loss_rpn_box_reg: 0.0110 (0.0110)  time: 1.9096  data: 1.2790  max mem: 8082\n",
      "Epoch: [3]  [100/822]  eta: 0:07:25  lr: 0.001000  loss: 0.4505 (0.4592)  loss_classifier: 0.0836 (0.0810)  loss_box_reg: 0.0412 (0.0423)  loss_mask: 0.2892 (0.3142)  loss_objectness: 0.0134 (0.0147)  loss_rpn_box_reg: 0.0059 (0.0071)  time: 0.5991  data: 0.0097  max mem: 8082\n",
      "Epoch: [3]  [200/822]  eta: 0:06:24  lr: 0.001000  loss: 0.4607 (0.4577)  loss_classifier: 0.0754 (0.0802)  loss_box_reg: 0.0414 (0.0427)  loss_mask: 0.3058 (0.3113)  loss_objectness: 0.0121 (0.0156)  loss_rpn_box_reg: 0.0068 (0.0078)  time: 0.5756  data: 0.0097  max mem: 8082\n",
      "Epoch: [3]  [300/822]  eta: 0:05:23  lr: 0.001000  loss: 0.4722 (0.4634)  loss_classifier: 0.0850 (0.0807)  loss_box_reg: 0.0412 (0.0425)  loss_mask: 0.3217 (0.3149)  loss_objectness: 0.0101 (0.0172)  loss_rpn_box_reg: 0.0059 (0.0081)  time: 0.6424  data: 0.0112  max mem: 8082\n",
      "Epoch: [3]  [400/822]  eta: 0:04:21  lr: 0.001000  loss: 0.3850 (0.4618)  loss_classifier: 0.0760 (0.0809)  loss_box_reg: 0.0406 (0.0430)  loss_mask: 0.2429 (0.3133)  loss_objectness: 0.0097 (0.0167)  loss_rpn_box_reg: 0.0048 (0.0080)  time: 0.6104  data: 0.0100  max mem: 8082\n",
      "Epoch: [3]  [500/822]  eta: 0:03:18  lr: 0.001000  loss: 0.4463 (0.4634)  loss_classifier: 0.0792 (0.0811)  loss_box_reg: 0.0404 (0.0432)  loss_mask: 0.3107 (0.3143)  loss_objectness: 0.0107 (0.0165)  loss_rpn_box_reg: 0.0064 (0.0084)  time: 0.5923  data: 0.0092  max mem: 8082\n",
      "Epoch: [3]  [600/822]  eta: 0:02:17  lr: 0.001000  loss: 0.3893 (0.4577)  loss_classifier: 0.0707 (0.0808)  loss_box_reg: 0.0355 (0.0432)  loss_mask: 0.2493 (0.3093)  loss_objectness: 0.0108 (0.0162)  loss_rpn_box_reg: 0.0048 (0.0083)  time: 0.6273  data: 0.0100  max mem: 8082\n",
      "Epoch: [3]  [700/822]  eta: 0:01:15  lr: 0.001000  loss: 0.4216 (0.4577)  loss_classifier: 0.0765 (0.0805)  loss_box_reg: 0.0421 (0.0432)  loss_mask: 0.2895 (0.3096)  loss_objectness: 0.0156 (0.0162)  loss_rpn_box_reg: 0.0050 (0.0083)  time: 0.5861  data: 0.0093  max mem: 8082\n",
      "Epoch: [3]  [800/822]  eta: 0:00:13  lr: 0.001000  loss: 0.4277 (0.4557)  loss_classifier: 0.0871 (0.0805)  loss_box_reg: 0.0438 (0.0431)  loss_mask: 0.2758 (0.3080)  loss_objectness: 0.0122 (0.0160)  loss_rpn_box_reg: 0.0049 (0.0081)  time: 0.7129  data: 0.0170  max mem: 8082\n",
      "Epoch: [3]  [821/822]  eta: 0:00:00  lr: 0.001000  loss: 0.4366 (0.4552)  loss_classifier: 0.0717 (0.0805)  loss_box_reg: 0.0435 (0.0432)  loss_mask: 0.2816 (0.3075)  loss_objectness: 0.0117 (0.0159)  loss_rpn_box_reg: 0.0050 (0.0080)  time: 0.5766  data: 0.0127  max mem: 8082\n",
      "Epoch: [3] Total time: 0:08:29 (0.6204 s / it)\n",
      "Epoch: [4]  [  0/822]  eta: 0:41:47  lr: 0.001000  loss: 0.5599 (0.5599)  loss_classifier: 0.0941 (0.0941)  loss_box_reg: 0.0436 (0.0436)  loss_mask: 0.3683 (0.3683)  loss_objectness: 0.0277 (0.0277)  loss_rpn_box_reg: 0.0262 (0.0262)  time: 3.0503  data: 2.2980  max mem: 8082\n",
      "Epoch: [4]  [100/822]  eta: 0:07:43  lr: 0.001000  loss: 0.3950 (0.4198)  loss_classifier: 0.0697 (0.0771)  loss_box_reg: 0.0441 (0.0437)  loss_mask: 0.2618 (0.2800)  loss_objectness: 0.0078 (0.0121)  loss_rpn_box_reg: 0.0041 (0.0069)  time: 0.6001  data: 0.0100  max mem: 8082\n",
      "Epoch: [4]  [200/822]  eta: 0:06:30  lr: 0.001000  loss: 0.3904 (0.4293)  loss_classifier: 0.0779 (0.0767)  loss_box_reg: 0.0418 (0.0428)  loss_mask: 0.2559 (0.2902)  loss_objectness: 0.0137 (0.0125)  loss_rpn_box_reg: 0.0052 (0.0071)  time: 0.6121  data: 0.0097  max mem: 8082\n",
      "Epoch: [4]  [300/822]  eta: 0:05:27  lr: 0.001000  loss: 0.4449 (0.4353)  loss_classifier: 0.0803 (0.0777)  loss_box_reg: 0.0413 (0.0426)  loss_mask: 0.2934 (0.2943)  loss_objectness: 0.0092 (0.0130)  loss_rpn_box_reg: 0.0054 (0.0076)  time: 0.6584  data: 0.0092  max mem: 8082\n",
      "Epoch: [4]  [400/822]  eta: 0:04:24  lr: 0.001000  loss: 0.4045 (0.4337)  loss_classifier: 0.0780 (0.0781)  loss_box_reg: 0.0409 (0.0429)  loss_mask: 0.2692 (0.2923)  loss_objectness: 0.0104 (0.0130)  loss_rpn_box_reg: 0.0050 (0.0075)  time: 0.6198  data: 0.0092  max mem: 8082\n",
      "Epoch: [4]  [500/822]  eta: 0:03:20  lr: 0.001000  loss: 0.4270 (0.4313)  loss_classifier: 0.0759 (0.0778)  loss_box_reg: 0.0415 (0.0425)  loss_mask: 0.2750 (0.2903)  loss_objectness: 0.0108 (0.0133)  loss_rpn_box_reg: 0.0058 (0.0074)  time: 0.6178  data: 0.0092  max mem: 8082\n",
      "Epoch: [4]  [600/822]  eta: 0:02:18  lr: 0.001000  loss: 0.4139 (0.4282)  loss_classifier: 0.0796 (0.0778)  loss_box_reg: 0.0396 (0.0422)  loss_mask: 0.2562 (0.2872)  loss_objectness: 0.0101 (0.0136)  loss_rpn_box_reg: 0.0055 (0.0074)  time: 0.5999  data: 0.0093  max mem: 8082\n",
      "Epoch: [4]  [700/822]  eta: 0:01:16  lr: 0.001000  loss: 0.4023 (0.4321)  loss_classifier: 0.0696 (0.0777)  loss_box_reg: 0.0422 (0.0420)  loss_mask: 0.2889 (0.2909)  loss_objectness: 0.0084 (0.0137)  loss_rpn_box_reg: 0.0056 (0.0079)  time: 0.5860  data: 0.0095  max mem: 8082\n",
      "Epoch: [4]  [800/822]  eta: 0:00:13  lr: 0.001000  loss: 0.3912 (0.4322)  loss_classifier: 0.0686 (0.0773)  loss_box_reg: 0.0413 (0.0420)  loss_mask: 0.2659 (0.2914)  loss_objectness: 0.0101 (0.0137)  loss_rpn_box_reg: 0.0047 (0.0078)  time: 0.5941  data: 0.0105  max mem: 8082\n",
      "Epoch: [4]  [821/822]  eta: 0:00:00  lr: 0.001000  loss: 0.3936 (0.4313)  loss_classifier: 0.0752 (0.0773)  loss_box_reg: 0.0375 (0.0419)  loss_mask: 0.2731 (0.2908)  loss_objectness: 0.0076 (0.0136)  loss_rpn_box_reg: 0.0049 (0.0077)  time: 0.5813  data: 0.0091  max mem: 8082\n",
      "Epoch: [4] Total time: 0:08:30 (0.6216 s / it)\n",
      "Epoch: [5]  [  0/822]  eta: 0:28:16  lr: 0.000100  loss: 0.3703 (0.3703)  loss_classifier: 0.0688 (0.0688)  loss_box_reg: 0.0223 (0.0223)  loss_mask: 0.2557 (0.2557)  loss_objectness: 0.0165 (0.0165)  loss_rpn_box_reg: 0.0070 (0.0070)  time: 2.0639  data: 1.3882  max mem: 8082\n",
      "Epoch: [5]  [100/822]  eta: 0:07:37  lr: 0.000100  loss: 0.3374 (0.3920)  loss_classifier: 0.0625 (0.0691)  loss_box_reg: 0.0347 (0.0376)  loss_mask: 0.2454 (0.2672)  loss_objectness: 0.0105 (0.0115)  loss_rpn_box_reg: 0.0052 (0.0066)  time: 0.5756  data: 0.0096  max mem: 8082\n",
      "Epoch: [5]  [200/822]  eta: 0:06:30  lr: 0.000100  loss: 0.4006 (0.4000)  loss_classifier: 0.0692 (0.0709)  loss_box_reg: 0.0396 (0.0388)  loss_mask: 0.2779 (0.2695)  loss_objectness: 0.0075 (0.0123)  loss_rpn_box_reg: 0.0047 (0.0085)  time: 0.6766  data: 0.0098  max mem: 8082\n",
      "Epoch: [5]  [300/822]  eta: 0:05:24  lr: 0.000100  loss: 0.3850 (0.3955)  loss_classifier: 0.0693 (0.0706)  loss_box_reg: 0.0382 (0.0387)  loss_mask: 0.2349 (0.2662)  loss_objectness: 0.0080 (0.0121)  loss_rpn_box_reg: 0.0055 (0.0078)  time: 0.6597  data: 0.0142  max mem: 8082\n",
      "Epoch: [5]  [400/822]  eta: 0:04:20  lr: 0.000100  loss: 0.3855 (0.3899)  loss_classifier: 0.0744 (0.0709)  loss_box_reg: 0.0427 (0.0388)  loss_mask: 0.2423 (0.2607)  loss_objectness: 0.0080 (0.0117)  loss_rpn_box_reg: 0.0046 (0.0077)  time: 0.6450  data: 0.0094  max mem: 8082\n",
      "Epoch: [5]  [500/822]  eta: 0:03:19  lr: 0.000100  loss: 0.3928 (0.3906)  loss_classifier: 0.0609 (0.0713)  loss_box_reg: 0.0378 (0.0390)  loss_mask: 0.2667 (0.2613)  loss_objectness: 0.0082 (0.0115)  loss_rpn_box_reg: 0.0065 (0.0076)  time: 0.6409  data: 0.0096  max mem: 8082\n",
      "Epoch: [5]  [600/822]  eta: 0:02:17  lr: 0.000100  loss: 0.3686 (0.3903)  loss_classifier: 0.0615 (0.0712)  loss_box_reg: 0.0374 (0.0390)  loss_mask: 0.2376 (0.2614)  loss_objectness: 0.0072 (0.0113)  loss_rpn_box_reg: 0.0045 (0.0073)  time: 0.6345  data: 0.0099  max mem: 8082\n",
      "Epoch: [5]  [700/822]  eta: 0:01:15  lr: 0.000100  loss: 0.3564 (0.3877)  loss_classifier: 0.0623 (0.0709)  loss_box_reg: 0.0342 (0.0388)  loss_mask: 0.2466 (0.2594)  loss_objectness: 0.0076 (0.0114)  loss_rpn_box_reg: 0.0049 (0.0073)  time: 0.6552  data: 0.0091  max mem: 8082\n",
      "Epoch: [5]  [800/822]  eta: 0:00:13  lr: 0.000100  loss: 0.3529 (0.3873)  loss_classifier: 0.0646 (0.0708)  loss_box_reg: 0.0377 (0.0392)  loss_mask: 0.2381 (0.2589)  loss_objectness: 0.0095 (0.0112)  loss_rpn_box_reg: 0.0055 (0.0072)  time: 0.6828  data: 0.0094  max mem: 8082\n",
      "Epoch: [5]  [821/822]  eta: 0:00:00  lr: 0.000100  loss: 0.3545 (0.3873)  loss_classifier: 0.0676 (0.0708)  loss_box_reg: 0.0370 (0.0392)  loss_mask: 0.2442 (0.2588)  loss_objectness: 0.0063 (0.0112)  loss_rpn_box_reg: 0.0058 (0.0073)  time: 0.6155  data: 0.0112  max mem: 8082\n",
      "Epoch: [5] Total time: 0:08:33 (0.6243 s / it)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model_ft, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rle(img, width, height):\n",
    "    rle = []\n",
    "    lastColor = 0\n",
    "    currentPixel = 0\n",
    "    runStart = -1\n",
    "    runLength = 0\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            currentColor = img[x][y]\n",
    "            if currentColor != lastColor:\n",
    "                if currentColor == 1:\n",
    "                    runStart = currentPixel\n",
    "                    runLength = 1\n",
    "                else:\n",
    "                    rle.append(str(runStart))\n",
    "                    rle.append(str(runLength))\n",
    "                    runStart = -1\n",
    "                    runLength = 0\n",
    "                    currentPixel = 0\n",
    "            elif runStart > -1:\n",
    "                runLength += 1\n",
    "            lastColor = currentColor\n",
    "            currentPixel+=1\n",
    "    return \" \" + \" \".join(rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform()\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d()\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign()\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu1): ReLU(inplace)\n",
       "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu2): ReLU(inplace)\n",
       "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu3): ReLU(inplace)\n",
       "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu4): ReLU(inplace)\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace)\n",
       "      (mask_fcn_logits): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n",
    "\n",
    "# this part was taken from @raddar's kernel: https://www.kaggle.com/raddar/better-sample-submission\n",
    "masks_ = sample_df.groupby('ImageId')['ImageId'].count().reset_index(name='N')\n",
    "masks_ = masks_.loc[masks_.N > 1].ImageId.values\n",
    "###\n",
    "sample_df = sample_df.drop_duplicates('ImageId', keep='last').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1372/1372 [01:33<00:00, 14.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tt = transforms.ToTensor()\n",
    "sublist = []\n",
    "counter = 0\n",
    "threshold = 0.3\n",
    "for index, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "    image_id = row['ImageId']\n",
    "    if image_id in masks_:\n",
    "        img_path = os.path.join(data_path, 'test_png', image_id + '.png')\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = img.size\n",
    "        img = img.resize((1024, 1024), resample=Image.BILINEAR)\n",
    "        img = tt(img)\n",
    "        result = model_ft([img.to(device)])[0]\n",
    "        if len(result[\"masks\"]) > 0:\n",
    "            counter += 1\n",
    "            mask_added = 0\n",
    "            for ppx in range(len(result[\"masks\"])):\n",
    "                if result[\"scores\"][ppx] >= threshold:\n",
    "                    mask_added += 1\n",
    "                    res = transforms.ToPILImage()(result[\"masks\"][ppx].permute(1, 2, 0).cpu().numpy())\n",
    "                    res = np.asarray(res.resize((width, height), resample=Image.BILINEAR))\n",
    "                    res = (res[:, :] * 255. > 127).astype(np.uint8).T\n",
    "                    rle = mask_to_rle(res, width, height)\n",
    "                    sublist.append([image_id, rle])\n",
    "            if mask_added == 0:\n",
    "                rle = \" -1\"\n",
    "                sublist.append([image_id, rle])\n",
    "        else:\n",
    "            rle = \" -1\"\n",
    "            sublist.append([image_id, rle])\n",
    "    else:\n",
    "        rle = \" -1\"\n",
    "        sublist.append([image_id, rle])\n",
    "\n",
    "submission_df = pd.DataFrame(sublist, columns=sample_df.columns.values)\n",
    "submission_df.to_csv(\"mrcnn_test.csv\", index=False)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python siim",
   "language": "python",
   "name": "siim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
